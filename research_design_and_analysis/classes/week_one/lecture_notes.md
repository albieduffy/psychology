# Research Design & Analysis

## 1. Core concepts

### Theory
A **theory** is a coherent, tentatively verified statement that explains relationships between constructs. Theories are abstract (you can’t directly observe them) and are supported by a network of propositions and assumptions.

### Assumption
An **assumption** is an untestable ‘given’ that underpins a theory. While theories are tentative, assumptions are treated as fixed for the purpose of deriving predictions.

### Construct
A **construct** is an abstract concept you want to study (e.g., *closeness*, *attention*, *anxiety*).

### Construct definition (conceptual)
A conceptual definition explains what the construct means in theory (e.g., closeness = perceived interpersonal connectedness and shared identity).

### Operationalisation / Operational definition
**Operationalisation** is how you convert a conceptual construct into measurable, manipulable procedures (e.g., using the Inclusion-of-Other-in-the-Self (IOS) pictorial scale, a 7‑item closeness questionnaire, or physical proximity in metres). An **operational definition** specifies the exact procedures, instructions, and scoring rules you will use.

### Hypothesis
A **hypothesis** is a specific, testable claim about the relationship between operationalised variables. It should link the theoretical construct to observable outcomes.

---

## 2. Why operational definitions matter (trade-offs & guidance)

- **Purpose**: they put abstract theory into concrete terms so others can measure, replicate and critique your work.
- **Trade-off**: Very specific operationalisations improve clarity and replicability but reduce generalisability. Broad operationalisations increase generality but risk ambiguity.
- **Best practice**:
  - Be explicit: give exact scripts, equipment, scoring rules, and exclusion rules.
  - Provide multiple measures (triangulation): e.g., self-report + behavioural + manipulation check.
  - Make materials and data public (open science) so readers can see exactly how you operationalised constructs.

**Operational-definition checklist** (for each variable):
1. Name of the variable (IV/DV/control)
2. Exact procedure or item wording
3. Scale and scoring method (range, higher = what?)
4. Time of measurement (pre/post/during)
5. Any transformation or calculation (averaging, reverse-scoring)
6. Exclusion rules or missing-data handling

---

## 3. What makes a *good* theory?
A good theory:
- Is **testable** and makes specific predictions
- **Accounts for existing data** and fits a variety of observations
- **Predicts future outcomes** and guides future research
- Is **parsimonious**: explains more with fewer assumptions
- Produces **verifiable** predictions (observable outcomes)
- Is **useful** (suggests interventions, clarifies mechanisms)

---

## 4. Reasoning styles in research

### Inductive reasoning (bottom-up)
- Move from specific observations to broader generalisations.
- Useful early in enquiry (case studies, paradoxes, practitioner heuristics).
- Strength: discovery. Weakness: conclusions can overreach the evidence.

### Deductive reasoning (top-down)
- Start with a theory or general premise and derive specific predictions.
- The hypothetico-deductive method sits here: theory → hypothesis → test.
- Strength: precise predictions and direct tests of theory.

**Tip**: both styles are valuable—use inductive work to generate hypotheses and deductive methods to test them.

---

## 5. Models of hypothesis testing (short)
- **Confirmation**: seek evidence that supports a hypothesis.
- **Disconfirmation**: seek evidence that would falsify a hypothesis.
- **Theory‑pitting / strong inference**: compare competing theories directly (which predicts the observed pattern better?).
- **Qualification (moderation)**: identify conditions where the hypothesis holds or fails ("works for X under Y conditions, not Z").

---

## 6. Perils & practical realities in psychology research
- Many psychological theories are verbally stated and loosely specified, which makes precise prediction difficult.
- Lack of experimental control (e.g., uncontrolled arousal) can produce spurious support.
- The vagueness of predictions ("Group A > Group B") makes false positives more likely; precise, quantitative predictions reduce this risk.
- Journals and publication incentives can bias towards novel, eye-catching results rather than reliable, replicable ones.

**Practical implication**: when you read a methods section, expect to *critically* examine manipulation precision, operational definitions and possible confounds.

---

## 7. Kuhn & paradigm shifts (concise)
- **Paradigm**: the shared assumptions, methods and standards of a scientific community.
- **Paradigm shift**: when accumulated anomalies cause a scientific revolution—old assumptions get overturned and new frameworks take their place.
- The take-away: dominant frameworks can persist even when contradictory evidence accumulates; be open to anomalies.

---

## 8. Goals of psychological research
- **Descriptive/exploratory**: map behaviour, find new phenomena.
- **Explanatory/theory-building**: accumulate evidence to build models that explain mechanisms.
- **Predictive**: make accurate predictions about behaviour in new situations.
- **Applied**: translate findings into interventions, policy, or practice.

Early work often mixes methods (qual + quant). At later stages, aim for clear tests of competing explanations.

---

## 9. Generating research ideas — heuristics
- Observe everyday life and popular culture.
- Read widely (across fields and classics).
- Replicate or question surprising findings.
- Look for contradictions or gaps in the literature.
- Translate practical problems into questions ("why does X happen?").

---

## 10. Validity (quick primer)
- **Internal validity**: are observed effects causal (did IV cause DV)? Control confounds, randomise, and use blinding where possible.
- **Construct validity**: are you really measuring the construct you claim? Use validated scales, multiple measures, and clear operational definitions.
- **External validity**: how well do findings generalise (populations, settings, times)? Use diverse samples and transparent boundary claims.

---

## 11. Methods critique checklist (when reading a paper)
- Is the hypothesis clear and falsifiable?
- Are constructs defined conceptually and operationally?
- Is the sample clearly described and appropriate?
- Are procedures scripted and replicable (materials, timings, instructions)?
- Are manipulation checks included and successful?
- How are exclusions handled and reported?
- Are the statistical analyses pre-specified or exploratory?
- Are effect sizes and confidence intervals reported (not just p-values)?
- Is data/code available or are materials described sufficiently for replication?

---

## 12. Open science best practice (practical tips)
- **Pre-register** the hypothesis, sample size, exclusions and analysis plan (OSF, AsPredicted, or similar).
- **Share materials & code** (anonymised data, scripts to reproduce analyses, survey items, stimulus files).
- Use **registered reports** or preprints where appropriate.
- Report all measures, conditions and exclusions transparently.

---

## 13. Practical study design: *Synchronised movement & closeness* (formative assessment)

### Research question
Which aspects of synchronised movement increase perceived closeness between partners? Specifically: does **mirroring** and **physical distance** affect self-reported closeness?

### Design (clear, tractable)
**2 × 2 between-subjects factorial**
- Factor A (Mirroring): *Mirroring* vs *No mirroring* (participants either mirror each other’s movements or perform similar movements without mirroring).
- Factor B (Distance): *Close* (≈1 m) vs *Far* (≈3 m).

**Why this design?** It is simple, easy to run in a lab or classroom setting, amenable to ANOVA, and avoids carry-over effects that would complicate mirroring manipulations.

### Sample
- Aim for **~30 participants per cell** as a practical student guideline (2×2×30 = **120 participants**). This gives reasonable sensitivity for medium-sized effects in student projects.
- If you cannot reach N=120, consider within-subjects or mixed designs (but be careful of order effects; counterbalance and include long breaks).

### Participants
- Recruit adult volunteers (clear inclusion/exclusion criteria). If university students, note limits on generalisability.
- Pair participants with unfamiliar partners (or use confederates trained to be consistent).

### IV operational definitions
- **Mirroring condition**: pairs are instructed to *mirror* each other’s movements ("copy the partner's movements exactly, like a mirror for 3 minutes"). Provide practice and a short script.
- **No‑mirroring condition**: pairs are instructed to perform a set of movements synchronously but not mirror (e.g., both follow a choreography or both copy a recorded instructor, which removes dyadic mirroring but keeps synchrony to some degree).
- **Close**: average interpersonal distance maintained at ~1 metre.
- **Far**: average interpersonal distance maintained at ~3 metres.
- *Standardise movement content* (basic arm and torso movements, tempo) and timing (e.g., 3 minutes of movement with a metronome or background music).

### DV operational definitions (primary & secondary)
- **Primary DV — subjective closeness**: Inclusion-of-Other-in-the-Self (IOS) pictorial scale (single-item, clear visuals). Score range and interpretation should be specified.
- **Secondary DV — self-report closeness questionnaire**: 5-item Likert-type scale (e.g., "I felt close to my partner" 1–7). Average items to create a scale (report reliability, e.g., Cronbach’s α).
- **Behavioural DV (optional)**: seating choice in a post-task filler task (do they choose to sit closer to partner?) or willingness to share small resources.
- **Manipulation checks**: perceived mirroring ("To what extent did your partner mirror your movements?") and perceived distance.
- **Control measures**: baseline mood, trait sociability, and familiarity with partner (should be low/recorded).

### Procedure (step-by-step)
1. Consent and baseline questionnaires (demographics, baseline closeness, mood)
2. Random assignment to one of four conditions
3. Short training for the movement task and practice trial
4. 3-minute movement task under the assigned condition (record video for later coding if feasible)
5. Immediate post-task measures: IOS, closeness questionnaire, manipulation checks, mood/arousal measure
6. Behavioural filler task (e.g., seating choice or resource allocation)
7. Debrief and (if used) compensation

### Ethics & practical considerations
- Get informed consent and include explicit statements about physical proximity and touch (if any). Offer the option to withdraw at any time.
- If touch is included, ensure participants can opt out, and follow institutional guidance (COVID precautions, hygiene, distance rules).
- Protect privacy when recording video; get explicit consent and store files securely.

### Planned analysis
- **Primary analysis**: 2×2 ANOVA on IOS scores (main effects of Mirroring and Distance + interaction).
- **Secondary**: ANOVA on the self-report scale; chi-square or logistic regression for behavioural measures.
- **Checks**: test assumptions (normality, homogeneity of variance). If violated, use robust alternatives or transform data.
- **Report**: F-statistics, p-values, partial η² or Cohen’s d for effects, and 95% confidence intervals. Include descriptive statistics (means, SDs) for each cell.
- **Exploratory analyses**: mediation (e.g., did arousal or synchrony mediate the effect?) — label these clearly as exploratory.

### Hypotheses (example)
- **Directional**: Participants in the *mirroring* condition will report greater closeness (higher IOS scores) than those in the non-mirroring condition.
- **Directional**: Participants in the *close distance* condition will report greater closeness than those in the far distance condition.
- **Interaction exploratory**: Mirroring will have a stronger effect when participants are physically closer.

### Practical materials & scripts (short examples)
- **Consent blurb (short)**: "You will take part in a brief movement task with another participant. You may stop at any time. We will ask you to complete short questionnaires afterwards. Your data are anonymous."

- **Mirroring instruction (script)**: "For 3 minutes, copy your partner’s movements exactly, as if they were a reflection in a mirror. Keep the same speed and timing."

- **No‑mirroring instruction**: "For 3 minutes, both of you will follow the instructor's movements. Try to perform the movements in time with the instructor, but do not mirror each other."

---

## 14. Quick tips for writing up methods (and the VLE formative post)
- Keep it concise but precise: include exact timings, scripts, and measures.
- State pre-registration status (if pre-registered mention where).
- Include a summary paragraph that a reader can use to *replicate* the procedure.
- Mention ethical approval and debriefing procedures.
- For the formative (non-graded) post, include a short request for feedback on feasibility, likely confounds, and sample size.

---

## 15. Short reading/critique practice (how to approach papers in future)
- Read the methods first: can you *recreate* the study from the text? If not, that’s a red flag.
- Ask: which constructs are ambiguous and how might alternative operationalisations affect results?
- Always check for manipulation checks and whether they actually validated the manipulation.

---

## 16. Final practical checklist before you run your study
- [ ] Full scripts for instructions & training recorded
- [ ] Clear operational definitions for all variables
- [ ] Manipulation checks planned
- [ ] Pre-registration or clear analysis plan
- [ ] Ethical consent form and procedures ready
- [ ] Pilot tested (timing, clarity of instructions, reliability of scales)
- [ ] Data storage & anonymisation plan

---
